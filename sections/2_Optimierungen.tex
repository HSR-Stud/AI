\section{Optimierung}
\subsection{NIM-Spiel / Dame}

\subsection{Lagrange}
Lagrange Multiplikatoren definieren Nebenbedingungen:\\
\begin{tabular}{l l l}
	{$ h(x,\lambda) = f + \sum\limits_{k=1}^s{\lambda_k g_k} $ } &
		$s$:    &Anzahl Nebenbedingungen \\
		&$f(x)$:&Zu optimierende Funktion \\
		&$g_k$: &Nebenbedingungen mit $g_k(x) = 0$
\end{tabular}
\subsubsection{Beispiel}
Auf der Funktion $f(x,y) = x^2 + y^2$ ist der tiefste Punkt zu finden,
für den gilt: $y = 3x - 5$ \\
$ h(x,y,\lambda) = x^2 + y^2 + \lambda(y - 3x + 5) $ \\
Partielle Ableitungen $h'(x,y,\lambda)$ Null setzen
\begin{tabular}{ll}
	$f'(x)$&$ = 2 x + 3 \lambda$ \\
	$f'(y)$&$ = 2 y + \lambda$ \\
	$f'(\lambda)$&$ = y - 3x + 5$
\end{tabular}

Dieses Gleichungssystem kann dann ganz normal aufgelöst werden.

\subsection{Lineare Optimierung}
\subsubsection{Gradientenabstieg} \label{2_Gradientenabstieg}
\begin{enumerate}
	\item Startpunkt wählen
	\item Entlang des steilsten Abstiegs entlang bis Steigung zunimmt
	\item Neu ausrichten und wiederholen
	\item Fertig wenn Gradient nach unten zeigt
\end{enumerate}
Der Gradientenabstieg kann nur funktionieren wenn es keine lokalen
Minimas gibt. Existieren lokale Minimas muss ``gerüttelt'' werden, um
aus dem lokalen Minima auszubrechen.

Das Lernen eines Perzeptrons entspricht einem Gradientenabstieg

